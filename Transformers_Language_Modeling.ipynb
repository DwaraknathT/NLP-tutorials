{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAIjl5TOEHVb"
      },
      "source": [
        "# What is this notebook? \n",
        "This is my attempt to write a _clear_ and _understandable_ tutorial on language modeling using Transformers. \n",
        "\n",
        "Why another tutorial when everybody and their cats are writing Transformer and language modeling tutorials? Every tutorial I found skips on some important explanations that I had to think about to understand. How exactly does masking work in self attention? How is a vocabulary buit? How the heck does language generation actually work during inference time? Jut given a raw corpus, how can I build a language generator? Every existing codebase is too complicated with layers of abstractions that I am too lazy to parse through. \n",
        "\n",
        "Admittedly, this might not be the best of the best out there, but I try to build stuff from scratch to get a minimal understanding of everything. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJZ5_0jpXs7L"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "38aDSZp4EKJZ"
      },
      "outputs": [],
      "source": [
        "import math \n",
        "import torch \n",
        "import numpy as np \n",
        "from torchtext.datasets import WikiText2\n",
        "from collections import Counter, OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LER6xmVYRka-",
        "outputId": "5b9859c6-486c-457c-cf15-4b316c0329ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.48M/4.48M [00:00<00:00, 19.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "def tokenize(sentence):\n",
        "  # Simple white space tokenizer \n",
        "  return sentence.strip().split()\n",
        "  \n",
        "# Tokenize each document in the training set \n",
        "# WikiText2 has a bunch of documents, each document has some x number of tokens \n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "tokenized_train_iter = map(tokenize, train_iter)\n",
        "tokenized_val_iter = map(tokenize, val_iter)\n",
        "tokenized_test_iter = map(tokenize, test_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qZd_-ylPe4TE"
      },
      "outputs": [],
      "source": [
        "min_freq = 10 # minimum freqency of occurance to be included in the vocab \n",
        "special_tokens = [\"<unk>\"]\n",
        "\n",
        "# Simple vocabulary class \n",
        "class vocab():\n",
        "  \"\"\"\n",
        "  A simple vocab class to build a vocab from an interator of tokens. \n",
        "  To the point, what am I doing here \n",
        "    * Make a set of words from the iterator \n",
        "    * Count how many times each word occurs \n",
        "    * Sort them based on their count \n",
        "    * If a word occurs more than some x times, add it to vocab \n",
        "    * Pick top k if you have a k in mind \n",
        "    * Add special tokens and make <unk> as default for words not in vocab \n",
        "    * Have functions to convert ids to tokens and vice versa \n",
        "  \"\"\"\n",
        "  def __init__(self, iterator, special_tokens=None):\n",
        "    self.default_key = 0 \n",
        "    counter = Counter() \n",
        "    # Count the number of occurances in each token \n",
        "    for token in iterator:\n",
        "      counter.update(token) \n",
        "    # Sort the counter and then filter \n",
        "    sorted_counter = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
        "    # Make it an ordered dict \n",
        "    sorted_counter = OrderedDict(sorted_counter) \n",
        "\n",
        "    # Pop the special tokens from counter if they already exist \n",
        "    token_set = []\n",
        "    if special_tokens:\n",
        "      for special_token in special_tokens:\n",
        "        sorted_counter.pop(special_token, None)\n",
        "        token_set.append(special_token)\n",
        "\n",
        "    # If greater than min freq then add to token set \n",
        "    for token, count in sorted_counter.items():\n",
        "      if count >= min_freq:\n",
        "        token_set.append(token)\n",
        "    \n",
        "    self.vocab_size = len(token_set)\n",
        "    self.token2id_map = {token:id for id, token in enumerate(token_set)}\n",
        "    self.id2token_map = {id:token for id, token in enumerate(token_set)}\n",
        "\n",
        "  def itos(self, indices):\n",
        "    # Function to convert a list of indices to list of tokens \n",
        "    return [self.id2token_map.get(idx) for idx in indices]\n",
        "  \n",
        "  def stoi(self, token_list):\n",
        "    # Function to convert token list to list of indices.\n",
        "    # Default to 0 if it's not in voab \n",
        "    return [self.token2id_map.get(token, 0) for token in token_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I3LO3jy5faOQ"
      },
      "outputs": [],
      "source": [
        "v = vocab(tokenized_train_iter, special_tokens)\n",
        "vocab_size = v.vocab_size "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yEaaFJfjil_Z"
      },
      "outputs": [],
      "source": [
        "# The iterators are consumed, load again \n",
        "# WikiText2 has a bunch of documents, each document has some x number of tokens \n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "tokenized_train_iter = map(tokenize, train_iter)\n",
        "tokenized_val_iter = map(tokenize, val_iter)\n",
        "tokenized_test_iter = map(tokenize, test_iter)\n",
        "\n",
        "# convert each token in dataset to tensor \n",
        "train_tokens, test_tokens, val_tokens = [], [], [] \n",
        "for sample in tokenized_train_iter:\n",
        "  train_tokens.append(torch.tensor(v.stoi(sample), dtype=torch.long))\n",
        "\n",
        "for sample in tokenized_val_iter:\n",
        "  val_tokens.append(torch.tensor(v.stoi(sample), dtype=torch.long))\n",
        "\n",
        "for sample in tokenized_test_iter:\n",
        "  test_tokens.append(torch.tensor(v.stoi(sample), dtype=torch.long))\n",
        "\n",
        "# Combine all train documents a single long set of tokens. \n",
        "# Imp Note : the order of the tokens must be preserved \n",
        "train_set = torch.cat(tuple(filter(lambda t: t.numel() > 0, train_tokens)))\n",
        "val_set = torch.cat(tuple(filter(lambda t: t.numel() > 0, val_tokens)))\n",
        "test_set = torch.cat(tuple(filter(lambda t: t.numel() > 0, test_tokens)))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def batchify(data, bsz):\n",
        "  \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "  that wouldn't cleanly fit.\n",
        "\n",
        "  Args:\n",
        "      data: Tensor, shape [N]\n",
        "      bsz: int, batch size\n",
        "\n",
        "  Returns:\n",
        "      Tensor of shape [N // bsz, bsz]\n",
        "  \"\"\"\n",
        "  seq_len = data.size(0) // bsz\n",
        "  data = data[:seq_len * bsz]\n",
        "  data = data.view(bsz, seq_len).t().contiguous()\n",
        "  return data.to(device)\n",
        "  \n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_set, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_set, eval_batch_size) \n",
        "test_data = batchify(test_set, eval_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyz9ZWqLGigJ"
      },
      "source": [
        "# Batching \n",
        "Alright, this is one thing that I was confused about in the official pytorch language modeling tutorial. How exactly is the dataset being partitioned and why the heck is batch size so low? I found the diagram of 1D convolution an easy way to understand what's happening here. \n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcsNZEOWKz3y"
      },
      "source": [
        "Let's say we have a looooong sequence of tokens, a language model basically looks at a sequence of tokens and predicts the next token, right? But the question is, how long back in history should we go back? Going back all the way to the starting of the sequence might make learning hard if the sequence is super long. Also, the self attention mechanism complexity is O(n^{2}). It rises up _super_ quickly. So we take a window. We slide that window over the sequence and predict the next word for each context. \n",
        "\n",
        "For example, let's say our sequence is A B C D E F G, and out window size is 3. Our first sample is A B C and the correspondings targets are B C D. Given A, predict B. Given A B, predict C. Given A B C, predict D. How do we impart this autoregressive property? More on that later. \n",
        "\n",
        "Back to Batching: We can improve the speed of our training by breaking the entire sequence into multiple parallel sequence. We los the connection between sequences, but that's relatively small compared to the dataset sizes we have. This interpretation sort of swaps the axes. For example, usually, our input tensors will have the shape (batch_size, input dims**). Each element in the batch_size dimension is independent. \n",
        "\n",
        "Here, our input tensors have the shape (bptt_window_size, batch_size). The time dimension comes first, and elements in the first dimension are not independent of each other. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "5D54rp3wsGDz"
      },
      "outputs": [],
      "source": [
        "# Let's define the transformer stuff, starting with multiheaded attention  \n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "\n",
        "q_dim = 256\n",
        "v_dim = 512\n",
        "emb_dim = 256\n",
        "n_heads = 4 \n",
        "hidden_dim = 512\n",
        "\n",
        "# Let's try to understand what's happening here \n",
        "def scaled_dot_prod_attention(q, k, v, mask=None):\n",
        "  \"\"\"\n",
        "  shapes of the incoming tensors \n",
        "  q = [batch_size, num_heads, max_seq_len_t, q_dim] \n",
        "  k = [batch_size, num_heads, max_seq_len_i, k_dim]\n",
        "  v = [batch_size, num_heads, max_seq_len_i, v_dim]\n",
        "  \n",
        "  Usually, k_dim and q_dim are equal, as we need to perform the dot product. \n",
        "  Also, max_seq_len of source and target need not be same.\n",
        "  That is, the attention scores need not be a square matrix. In the general case it is of shape [max_seq_len, max_seq_len]\n",
        "  However, we are training, say, a machine translation model, these lengths won't be same.\n",
        "  So, in summary, V and K must have same number of tokens.\n",
        "\n",
        "  The decoder of the transformer is a general case that is applicable in all scenarios \n",
        "  In decoder self attention, we use the target vector as q, k and v. Because we want to\n",
        "  find how each token in the target is related to every other token. \n",
        "  During the encoder-decoder self attention, we want the q to be source tokens, k to be target tokens, \n",
        "  because we want to know how each token in the source is related to target. Then use those scores to update the\n",
        "  representaiton of target. \n",
        "\n",
        "  Let's ignore batchsize and num_heads for now. \n",
        "  q = [seq_len_t, q_dim]\n",
        "  k = [seq_len_i, k_dim] \n",
        "  v = [seq_len_i, v_dim]\n",
        "\n",
        "  q_dim = k_dim \n",
        "  so attention scores = [seq_len_t, seq_len_i]\n",
        "  output = scores * v = [seq_len_t, seq_len_i] * [seq_len_i, hidden_dim]\n",
        "  So for each dimension in hidden_dim, we take the weighted sum of input based on attention scores \n",
        "  \"\"\"\n",
        "  k_dim = k.shape[-1] \n",
        "  dot_prods = torch.matmul(q, torch.transpose(k, -2, -1)) # shape - [max_seq_len_i, max_seq_len_t]\n",
        "  # The above dot product might push values high really quicky, and thus enter regions in\n",
        "  # softmax where the gradients are not that informative. So we divide with dimension to scale \n",
        "  dot_prods /= math.sqrt(k_dim) \n",
        "  \"\"\"\n",
        "  Now, another important topic is masking of attention weights during self attention. \n",
        "  Let's say we are training a language model, that is, predict the next word given all previous words. \n",
        "  During training time, we of course have the entire sentence with us, but we can't use information \n",
        "  from the future. So we mask out subsequent positions. \n",
        "\n",
        "  The mask variable passed into this function is of shape [max_seq_len, max_seq_len] \n",
        "  The matrix has 0s in positions that must be masked out.\n",
        "  Let's understand this with an example. \n",
        "  Let's say our input is [A B C D E] and our target is [B C D E F]. \n",
        "  Given A - Predict B \n",
        "  Given A B - predict C \n",
        "  Given A B C - predict D...\n",
        "  So we see a traingular matrix here. Our mask for that example is gonna be \n",
        "  [1 0 0 0 0] -> value of a - score of a * value of a \n",
        "  [1 1 0 0 0] -> value value of b - score of a * value of a + score of b * value of b \n",
        "  [1 1 1 0 0]\n",
        "  [1 1 1 1 0]\n",
        "  [1 1 1 1 1]\n",
        "  since we are adding this to the scores, our mask will be \n",
        "  [0 -inf -inf -inf]\n",
        "  [0 0 -inf -inf -inf]\n",
        "  ....\n",
        "  \"\"\"\n",
        "  if mask is not None:\n",
        "    dot_prods += mask \n",
        "  # use softmax to get the attention scores \n",
        "  attention_scores = F.softmax(dot_prods, dim=-1)\n",
        "  # Now take the weight sum of values based on scores \n",
        "  output = torch.matmul(attention_scores, v) \n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.q_dim = q_dim \n",
        "    self.v_dim = v_dim \n",
        "    self.emd_dim = emb_dim\n",
        "\n",
        "    # Linear projection layers \n",
        "    self.q_layer = nn.Linear(emb_dim, q_dim * n_heads)\n",
        "    self.v_layer = nn.Linear(emb_dim, v_dim * n_heads)\n",
        "    self.k_layer = nn.Linear(emb_dim, q_dim * n_heads)\n",
        "\n",
        "    # Final linear layer \n",
        "    self.linear = nn.Linear(v_dim * n_heads, emb_dim)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    # x shape - [batch_size, seq_len, emb_dim]\n",
        "    batch = x.shape[0]\n",
        "    seq_len = x.shape[1] \n",
        "    q = self.q_layer(x) # shape - [batch_size, seq_len, q_dim * n_heads]\n",
        "    k = self.k_layer(x) # shape - [batch_size, seq_len, q_dim * n_heads]\n",
        "    v = self.v_layer(x) # shape - [batch_size, seq_len, v_dim * n_heads]\n",
        "    # reshape to make heads another dim \n",
        "    q = q.reshape(batch, n_heads, seq_len, -1)\n",
        "    v = v.reshape(batch, n_heads, seq_len, -1)\n",
        "    k = k.reshape(batch, n_heads, seq_len, -1)\n",
        "    # multi headed attention \n",
        "    attention_output = scaled_dot_prod_attention(q, k, v, mask)\n",
        "    attention_output = attention_output.reshape(batch, seq_len, -1)\n",
        "    return self.linear(attention_output)\n",
        "  \n",
        "class LinearLayers(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LinearLayers, self).__init__() \n",
        "    self.linear1 = nn.Linear(emb_dim, hidden_dim)\n",
        "    self.linear2 = nn.Linear(hidden_dim, emb_dim)\n",
        "    self.act = nn.ReLU() \n",
        "    \n",
        "  def forward(self, x):\n",
        "    return self.linear2(self.act(self.linear1(x)))"
      ],
      "metadata": {
        "id": "F6HPF83gox2R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YFM50tTyAetG"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.mha = MultiHeadAttention() \n",
        "    self.attention_norm = nn.LayerNorm(emb_dim) \n",
        "    self.linear = LinearLayers() \n",
        "    self.linear_norm = nn.LayerNorm(emb_dim) \n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    input = x \n",
        "    output = self.attention_norm(self.mha(x, mask))\n",
        "    output = self.linear_norm(self.linear(output))\n",
        "    return output + input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "AkPxU8bdBft4"
      },
      "outputs": [],
      "source": [
        "n_layers = 4 \n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "    self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
        "    self.final_layer = nn.Linear(emb_dim, vocab_size)\n",
        "    self.pe = self.pos_encoding(bptt, emb_dim)\n",
        "\n",
        "  def pos_encoding(self, seq_len, d_model):\n",
        "    position = torch.arange(seq_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "    pe = torch.zeros(seq_len, 1, d_model)\n",
        "    pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "    return pe.transpose(0, 1).to(device)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    x = self.embedding(x) \n",
        "    # Add pos encoding \n",
        "    try:\n",
        "      x += self.pe[:, :x.size(1), :]\n",
        "    except:\n",
        "      print(x.shape, self.pe.shape)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask) \n",
        "    prob_logits = self.final_layer(x) \n",
        "    return prob_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "BqERQjILk8-B"
      },
      "outputs": [],
      "source": [
        "bptt = 512 # Backprop through time window \n",
        "\n",
        "def get_data(tensor, i, batch_first=True):\n",
        "  # take chunks out of tensor starting from index i \n",
        "  chunk_size = min(bptt, len(tensor) -i - 1) # The final chunk might have less number than bptt window \n",
        "  inputs = tensor[i: i+chunk_size]\n",
        "  # Flatten the target tensor, easy to calculate cross entropy loss\n",
        "  targets = tensor[i+1: i+1+chunk_size].reshape(-1) \n",
        "  if batch_first:\n",
        "    inputs = inputs.transpose(0, 1)\n",
        "  return inputs, targets \n",
        "\n",
        "def get_mask(size):\n",
        "  \"Mask out subsequent positions.\"\n",
        "  return torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KpdUjgDr7dQ"
      },
      "outputs": [],
      "source": [
        "# Now we have to write the training loop \n",
        "\n",
        "model = Transformer().to(device) \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "total_loss = 0 \n",
        "total_ppl = 0 \n",
        "log_interval = 50\n",
        "\n",
        "# Eval function to eval on dev set \n",
        "\n",
        "def eval():\n",
        "  step = 0 \n",
        "  total_loss = 0 \n",
        "  total_ppl = 0 \n",
        "  model.eval() \n",
        "  # Sample from eval set \n",
        "  with torch.no_grad():\n",
        "    for i in range(0, val_data.shape[0], bptt):\n",
        "      step += 1 \n",
        "      inputs, targets = get_data(val_data, i) \n",
        "      inputs, targets = inputs.to(device), targets.to(device) \n",
        "      mask = get_mask(min(bptt, len(val_data) -i -1)).to(device) \n",
        "\n",
        "      outputs = model(inputs, mask).reshape(-1, vocab_size)\n",
        "      loss = criterion(outputs, targets)\n",
        "      total_loss += loss.item() \n",
        "      total_ppl += math.exp(loss.item()) \n",
        "      avg_loss = total_loss / step \n",
        "      avg_ppl = total_ppl / step \n",
        "\n",
        "  print(\"Val: Avg Loss: {:.4f} Avg Ppl: {:.4f}\".format(avg_loss, avg_ppl))\n",
        "\n",
        "model.train() \n",
        "step = 0 \n",
        "# Loop from start of the dataset to the final index with increments of bptt window size \n",
        "for epoch in range(1, 101):\n",
        "  for i in range(0, train_data.shape[0], bptt):\n",
        "    step += 1 \n",
        "    # Get inputs and targets \n",
        "    inputs, targets = get_data(train_data, i)\n",
        "    # Move to device \n",
        "    inputs, targets = inputs.to(device), targets.to(device) \n",
        "    mask = get_mask(min(bptt, len(train_data) -i - 1)).to(device) \n",
        "\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    outputs = model(inputs, mask).reshape(-1, vocab_size)\n",
        "    loss = criterion(outputs, targets) \n",
        "\n",
        "    # backward pass \n",
        "    loss.backward() \n",
        "    optimizer.step()\n",
        "    \n",
        "    total_loss += loss.item() \n",
        "    total_ppl += math.exp(loss.item())\n",
        "    if step % log_interval == 0:\n",
        "      avg_loss = total_loss / step \n",
        "      avg_ppl = total_ppl / step \n",
        "      print(\"Train: Step: {} Avg Loss: {:.4f} Avg Ppl: {:.4f} \".format(step, avg_loss, avg_ppl))\n",
        "      eval() # Evalvulate on val set \n",
        "  print(\"-----------\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Transformers Language Modeling",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}