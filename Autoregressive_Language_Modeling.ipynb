{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAIjl5TOEHVb"
      },
      "source": [
        "# What is this notebook? \n",
        "This is my attempt to write a _clear_ and _understandable_ tutorial on language modeling using Transformers. \n",
        "\n",
        "Why another tutorial when everybody and their cats are writing Transformer and language modeling tutorials? Every tutorial I found skips on some important explanations that I had to think about to understand. How exactly does masking work in self attention? How is a vocabulary buit? How the heck does language generation actually work during inference time? Jut given a raw corpus, how can I build a language generator? Every existing codebase is too complicated with layers of abstractions that I am too lazy to parse through. \n",
        "\n",
        "Admittedly, this might not be the best of the best out there, but I try to build stuff from scratch to get a minimal understanding of everything. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "tJZ5_0jpXs7L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5baee4f1-4072-4a0f-9e3d-f40abbcc1f5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar 10 07:09:11 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P0    58W / 149W |  11137MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "38aDSZp4EKJZ"
      },
      "outputs": [],
      "source": [
        "import math \n",
        "import torch \n",
        "import numpy as np \n",
        "from torchtext.datasets import WikiText2\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "LER6xmVYRka-"
      },
      "outputs": [],
      "source": [
        "def tokenize(sentence):\n",
        "  # Simple white space tokenizer \n",
        "  return sentence.strip().split()\n",
        "  \n",
        "# Tokenize each document in the training set \n",
        "# WikiText2 has a bunch of documents, each document has some x number of tokens \n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "tokenized_train_iter = map(tokenize, train_iter)\n",
        "tokenized_val_iter = map(tokenize, val_iter)\n",
        "tokenized_test_iter = map(tokenize, test_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "qZd_-ylPe4TE"
      },
      "outputs": [],
      "source": [
        "min_freq = 10 # minimum freqency of occurance to be included in the vocab \n",
        "special_tokens = [\"<unk>\", \"<pad>\"]\n",
        "\n",
        "# Simple vocabulary class \n",
        "class vocab():\n",
        "  \"\"\"\n",
        "  A simple vocab class to build a vocab from an interator of tokens. \n",
        "  To the point, what am I doing here \n",
        "    * Make a set of words from the iterator \n",
        "    * Count how many times each word occurs \n",
        "    * Sort them based on their count \n",
        "    * If a word occurs more than some x times, add it to vocab \n",
        "    * Pick top k if you have a k in mind \n",
        "    * Add special tokens and make <unk> as default for words not in vocab \n",
        "    * Have functions to convert ids to tokens and vice versa \n",
        "  \"\"\"\n",
        "  def __init__(self, iterator, special_tokens=None):\n",
        "    self.default_key = 0 \n",
        "    counter = Counter() \n",
        "    # Count the number of occurances in each token \n",
        "    for token in iterator:\n",
        "      counter.update(token) \n",
        "    # Sort the counter and then filter \n",
        "    sorted_counter = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
        "    # Make it an ordered dict \n",
        "    sorted_counter = OrderedDict(sorted_counter) \n",
        "\n",
        "    # Pop the special tokens from counter if they already exist \n",
        "    token_set = []\n",
        "    if special_tokens:\n",
        "      for special_token in special_tokens:\n",
        "        sorted_counter.pop(special_token, None)\n",
        "        token_set.append(special_token)\n",
        "\n",
        "    # If greater than min freq then add to token set \n",
        "    for token, count in sorted_counter.items():\n",
        "      if count >= min_freq:\n",
        "        token_set.append(token)\n",
        "    \n",
        "    self.vocab_size = len(token_set)\n",
        "    self.token2id_map = {token:id for id, token in enumerate(token_set)}\n",
        "    self.id2token_map = {id:token for id, token in enumerate(token_set)}\n",
        "\n",
        "  def itos(self, indices):\n",
        "    # Function to convert a list of indices to list of tokens \n",
        "    return [self.id2token_map.get(idx) for idx in indices]\n",
        "  \n",
        "  def stoi(self, token_list):\n",
        "    # Function to convert token list to list of indices.\n",
        "    # Default to 0 if it's not in voab \n",
        "    return [self.token2id_map.get(token, 0) for token in token_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "I3LO3jy5faOQ"
      },
      "outputs": [],
      "source": [
        "v = vocab(tokenized_train_iter, special_tokens)\n",
        "vocab_size = v.vocab_size "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "yEaaFJfjil_Z"
      },
      "outputs": [],
      "source": [
        "# The iterators are consumed, load again \n",
        "# WikiText2 has a bunch of documents, each document has some x number of tokens \n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "tokenized_train_iter = map(tokenize, train_iter)\n",
        "tokenized_val_iter = map(tokenize, val_iter)\n",
        "tokenized_test_iter = map(tokenize, test_iter)\n",
        "\n",
        "# convert each token in dataset to tensor \n",
        "train_tokens, test_tokens, val_tokens = [], [], [] \n",
        "for sample in tokenized_train_iter:\n",
        "  train_tokens.append(torch.tensor(v.stoi(sample), dtype=torch.long))\n",
        "\n",
        "for sample in tokenized_val_iter:\n",
        "  val_tokens.append(torch.tensor(v.stoi(sample), dtype=torch.long))\n",
        "\n",
        "for sample in tokenized_test_iter:\n",
        "  test_tokens.append(torch.tensor(v.stoi(sample), dtype=torch.long))\n",
        "\n",
        "# Combine all train documents a single long set of tokens. \n",
        "# Imp Note : the order of the tokens must be preserved \n",
        "train_set = torch.cat(tuple(filter(lambda t: t.numel() > 0, train_tokens)))\n",
        "val_set = torch.cat(tuple(filter(lambda t: t.numel() > 0, val_tokens)))\n",
        "test_set = torch.cat(tuple(filter(lambda t: t.numel() > 0, test_tokens)))\n",
        "\n",
        "def batchify(data, bsz):\n",
        "  \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "  that wouldn't cleanly fit.\n",
        "\n",
        "  Args:\n",
        "      data: Tensor, shape [N]\n",
        "      bsz: int, batch size\n",
        "\n",
        "  Returns:\n",
        "      Tensor of shape [N // bsz, bsz]\n",
        "  \"\"\"\n",
        "  seq_len = data.size(0) // bsz\n",
        "  data = data[:seq_len * bsz]\n",
        "  data = data.view(bsz, seq_len).t().contiguous()\n",
        "  return data.to(device)\n",
        "  \n",
        "\n",
        "batch_size = 32\n",
        "eval_batch_size = 32\n",
        "train_data = batchify(train_set, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_set, eval_batch_size) \n",
        "test_data = batchify(test_set, eval_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyz9ZWqLGigJ"
      },
      "source": [
        "# Batching \n",
        "Alright, this is one thing that I was confused about in the official pytorch language modeling tutorial. How exactly is the dataset being partitioned and why the heck is batch size so low? I found the diagram of 1D convolution an easy way to understand what's happening here. \n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcsNZEOWKz3y"
      },
      "source": [
        "Let's say we have a looooong sequence of tokens, a language model basically looks at a sequence of tokens and predicts the next token, right? But the question is, how long back in history should we go back? Going back all the way to the starting of the sequence might make learning hard if the sequence is super long. Also, the self attention mechanism complexity is O(n^{2}). It rises up _super_ quickly. So we take a window. We slide that window over the sequence and predict the next word for each context. \n",
        "\n",
        "For example, let's say our sequence is A B C D E F G, and out window size is 3. Our first sample is A B C and the correspondings targets are B C D. Given A, predict B. Given A B, predict C. Given A B C, predict D. How do we impart this autoregressive property? More on that later. \n",
        "\n",
        "Back to Batching: We can improve the speed of our training by breaking the entire sequence into multiple parallel sequence. We los the connection between sequences, but that's relatively small compared to the dataset sizes we have. This interpretation sort of swaps the axes. For example, usually, our input tensors will have the shape (batch_size, input dims**). Each element in the batch_size dimension is independent. \n",
        "\n",
        "Here, our input tensors have the shape (bptt_window_size, batch_size). The time dimension comes first, and elements in the first dimension are not independent of each other. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "5D54rp3wsGDz"
      },
      "outputs": [],
      "source": [
        "# Let's define the transformer stuff, starting with multiheaded attention  \n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "\n",
        "q_dim = 512\n",
        "v_dim = 512\n",
        "emb_dim = 256\n",
        "n_heads = 4 \n",
        "hidden_dim = 512\n",
        "\n",
        "# Let's try to understand what's happening here \n",
        "def scaled_dot_prod_attention(q, k, v, mask=None):\n",
        "  \"\"\"\n",
        "  shapes of the incoming tensors \n",
        "  q = [batch_size, num_heads, max_seq_len_t, q_dim] \n",
        "  k = [batch_size, num_heads, max_seq_len_i, k_dim]\n",
        "  v = [batch_size, num_heads, max_seq_len_i, v_dim]\n",
        "  \n",
        "  Usually, k_dim and q_dim are equal, as we need to perform the dot product. \n",
        "  Also, max_seq_len of source and target need not be same.\n",
        "  That is, the attention scores need not be a square matrix. In the general case it is of shape [max_seq_len, max_seq_len]\n",
        "  However, we are training, say, a machine translation model, these lengths won't be same.\n",
        "  So, in summary, V and K must have same number of tokens.\n",
        "\n",
        "  The decoder of the transformer is a general case that is applicable in all scenarios \n",
        "  In decoder self attention, we use the target vector as q, k and v. Because we want to\n",
        "  find how each token in the target is related to every other token. \n",
        "  During the encoder-decoder self attention, we want the q to be source tokens, k to be target tokens, \n",
        "  because we want to know how each token in the source is related to target. Then use those scores to update the\n",
        "  representaiton of target. \n",
        "\n",
        "  Let's ignore batchsize and num_heads for now. \n",
        "  q = [seq_len_t, q_dim]\n",
        "  k = [seq_len_i, k_dim] \n",
        "  v = [seq_len_i, v_dim]\n",
        "\n",
        "  q_dim = k_dim \n",
        "  so attention scores = [seq_len_t, seq_len_i]\n",
        "  output = scores * v = [seq_len_t, seq_len_i] * [seq_len_i, hidden_dim]\n",
        "  So for each dimension in hidden_dim, we take the weighted sum of input based on attention scores \n",
        "  \"\"\"\n",
        "  k_dim = k.shape[-1] \n",
        "  dot_prods = torch.matmul(q, torch.transpose(k, -2, -1)) # shape - [max_seq_len_i, max_seq_len_t]\n",
        "  # The above dot product might push values high really quicky, and thus enter regions in\n",
        "  # softmax where the gradients are not that informative. So we divide with dimension to scale \n",
        "  dot_prods /= math.sqrt(k_dim) \n",
        "  \"\"\"\n",
        "  Now, another important topic is masking of attention weights during self attention. \n",
        "  Let's say we are training a language model, that is, predict the next word given all previous words. \n",
        "  During training time, we of course have the entire sentence with us, but we can't use information \n",
        "  from the future. So we mask out subsequent positions. \n",
        "\n",
        "  The mask variable passed into this function is of shape [max_seq_len, max_seq_len] \n",
        "  The matrix has 0s in positions that must be masked out.\n",
        "  Let's understand this with an example. \n",
        "  Let's say our input is [A B C D E] and our target is [B C D E F]. \n",
        "  Given A - Predict B \n",
        "  Given A B - predict C \n",
        "  Given A B C - predict D...\n",
        "  So we see a traingular matrix here. Our mask for that example is gonna be \n",
        "  [1 0 0 0 0] -> value of a - score of a * value of a \n",
        "  [1 1 0 0 0] -> value value of b - score of a * value of a + score of b * value of b \n",
        "  [1 1 1 0 0]\n",
        "  [1 1 1 1 0]\n",
        "  [1 1 1 1 1]\n",
        "  since we are adding this to the scores, our mask will be \n",
        "  [0 -inf -inf -inf]\n",
        "  [0 0 -inf -inf -inf]\n",
        "  ....\n",
        "  \"\"\"\n",
        "  if mask is not None:\n",
        "      dot_prods += mask \n",
        "  # use softmax to get the attention scores \n",
        "  attention_scores = F.softmax(dot_prods, dim=-1)\n",
        "  # Now take the weight sum of values based on scores \n",
        "  output = torch.matmul(attention_scores, v) \n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.q_dim = q_dim \n",
        "    self.v_dim = v_dim \n",
        "    self.emd_dim = emb_dim\n",
        "\n",
        "    # Linear projection layers \n",
        "    self.q_layer = nn.Linear(emb_dim, q_dim * n_heads)\n",
        "    self.v_layer = nn.Linear(emb_dim, v_dim * n_heads)\n",
        "    self.k_layer = nn.Linear(emb_dim, q_dim * n_heads)\n",
        "\n",
        "    # Final linear layer \n",
        "    self.linear = nn.Linear(v_dim * n_heads, emb_dim)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    # x shape - [batch_size, seq_len, emb_dim]\n",
        "    batch = x.shape[0]\n",
        "    seq_len = x.shape[1] \n",
        "    q = self.q_layer(x) # shape - [batch_size, seq_len, q_dim * n_heads]\n",
        "    k = self.k_layer(x) # shape - [batch_size, seq_len, q_dim * n_heads]\n",
        "    v = self.v_layer(x) # shape - [batch_size, seq_len, v_dim * n_heads]\n",
        "    # reshape to make heads another dim \n",
        "    q = q.reshape(batch, n_heads, seq_len, -1)\n",
        "    v = v.reshape(batch, n_heads, seq_len, -1)\n",
        "    k = k.reshape(batch, n_heads, seq_len, -1)\n",
        "    # multi headed attention \n",
        "    attention_output = scaled_dot_prod_attention(q, k, v, mask)\n",
        "    attention_output = attention_output.reshape(batch, seq_len, -1)\n",
        "    return self.linear(attention_output)\n",
        "  \n",
        "class LinearLayers(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LinearLayers, self).__init__() \n",
        "    self.linear1 = nn.Linear(emb_dim, hidden_dim)\n",
        "    self.linear2 = nn.Linear(hidden_dim, emb_dim)\n",
        "    self.act = nn.ReLU() \n",
        "    \n",
        "  def forward(self, x):\n",
        "    return self.linear2(self.act(self.linear1(x)))"
      ],
      "metadata": {
        "id": "F6HPF83gox2R"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "YFM50tTyAetG"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.mha = MultiHeadAttention() \n",
        "    self.attention_norm = nn.LayerNorm(emb_dim) \n",
        "    self.linear = LinearLayers() \n",
        "    self.linear_norm = nn.LayerNorm(emb_dim) \n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    input = x \n",
        "    output = self.attention_norm(self.mha(x, mask))\n",
        "    output = self.linear_norm(self.linear(output))\n",
        "    return output + input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "AkPxU8bdBft4"
      },
      "outputs": [],
      "source": [
        "n_layers = 4 \n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "    self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
        "    self.final_layer = nn.Linear(emb_dim, vocab_size)\n",
        "    self.pe = self.pos_encoding(bptt, emb_dim)\n",
        "\n",
        "  def pos_encoding(self, seq_len, d_model):\n",
        "    position = torch.arange(seq_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "    pe = torch.zeros(seq_len, 1, d_model)\n",
        "    pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "    return pe.transpose(0, 1).to(device)\n",
        "\n",
        "  def forward(self, x, attn_mask=None, pad_mask=None):\n",
        "    # The functionality of pad_mask is explained later on with an example \n",
        "    x = self.embedding(x) \n",
        "    # Add pos encoding \n",
        "    x += self.pe[:, :x.size(1), :]\n",
        "    if pad_mask is not None:\n",
        "      x = x.masked_fill_(pad_mask == 1, 0)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, attn_mask) \n",
        "    prob_logits = self.final_layer(x) \n",
        "    return prob_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "BqERQjILk8-B"
      },
      "outputs": [],
      "source": [
        "bptt = 512 # Backprop through time window \n",
        "\n",
        "def get_data(tensor, i, batch_first=True):\n",
        "  # take chunks out of tensor starting from index i \n",
        "  chunk_size = min(bptt, len(tensor) -i - 1) # The final chunk might have less number than bptt window \n",
        "  inputs = tensor[i: i+chunk_size]\n",
        "  # Flatten the target tensor, easy to calculate cross entropy loss\n",
        "  targets = tensor[i+1: i+1+chunk_size].reshape(-1) \n",
        "  if batch_first:\n",
        "    inputs = inputs.transpose(0, 1)\n",
        "  return inputs, targets \n",
        "\n",
        "def get_mask(size):\n",
        "  \"Mask out subsequent positions.\"\n",
        "  return torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "2KpdUjgDr7dQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2e4a9955-2be0-47e4-d56f-e23dd7aa715d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: Step: 50 Avg Loss: 7.2369 Avg Ppl: 2766.2410 \n",
            "Val: Avg Loss: 6.6930 Avg Ppl: 807.3660\n",
            "Train: Step: 100 Avg Loss: 7.0476 Avg Ppl: 1859.3099 \n",
            "Val: Avg Loss: 6.6526 Avg Ppl: 775.3015\n",
            "-----------\n",
            "Train: Step: 150 Avg Loss: 6.9535 Avg Ppl: 1529.4179 \n",
            "Val: Avg Loss: 6.6231 Avg Ppl: 752.8101\n",
            "Train: Step: 200 Avg Loss: 6.8836 Avg Ppl: 1345.0640 \n",
            "Val: Avg Loss: 6.6213 Avg Ppl: 751.3554\n",
            "Train: Step: 250 Avg Loss: 6.8424 Avg Ppl: 1234.9865 \n",
            "Val: Avg Loss: 6.6043 Avg Ppl: 738.8779\n",
            "-----------\n",
            "Train: Step: 300 Avg Loss: 6.7978 Avg Ppl: 1148.7446 \n",
            "Val: Avg Loss: 6.6229 Avg Ppl: 752.6985\n",
            "Train: Step: 350 Avg Loss: 6.7663 Avg Ppl: 1087.3766 \n",
            "Val: Avg Loss: 6.6158 Avg Ppl: 747.3677\n",
            "-----------\n",
            "Train: Step: 400 Avg Loss: 6.7379 Avg Ppl: 1038.1353 \n",
            "Val: Avg Loss: 6.6198 Avg Ppl: 750.2989\n",
            "Train: Step: 450 Avg Loss: 6.7124 Avg Ppl: 997.3534 \n",
            "Val: Avg Loss: 6.6217 Avg Ppl: 751.6746\n",
            "Train: Step: 500 Avg Loss: 6.6924 Avg Ppl: 965.0055 \n",
            "Val: Avg Loss: 6.6066 Avg Ppl: 740.4975\n",
            "-----------\n",
            "Train: Step: 550 Avg Loss: 6.6695 Avg Ppl: 934.4186 \n",
            "Val: Avg Loss: 6.6090 Avg Ppl: 742.2930\n",
            "Train: Step: 600 Avg Loss: 6.6519 Avg Ppl: 909.7389 \n",
            "Val: Avg Loss: 6.6515 Avg Ppl: 774.4648\n",
            "-----------\n",
            "Train: Step: 650 Avg Loss: 6.6335 Avg Ppl: 886.8596 \n",
            "Val: Avg Loss: 6.6202 Avg Ppl: 750.6094\n",
            "Train: Step: 700 Avg Loss: 6.6168 Avg Ppl: 866.5169 \n",
            "Val: Avg Loss: 6.6445 Avg Ppl: 769.0544\n",
            "Train: Step: 750 Avg Loss: 6.6033 Avg Ppl: 849.4515 \n",
            "Val: Avg Loss: 6.6209 Avg Ppl: 751.2138\n",
            "-----------\n",
            "Train: Step: 800 Avg Loss: 6.5865 Avg Ppl: 831.7850 \n",
            "Val: Avg Loss: 6.6617 Avg Ppl: 782.4005\n",
            "Train: Step: 850 Avg Loss: 6.5737 Avg Ppl: 817.1656 \n",
            "Val: Avg Loss: 6.6583 Avg Ppl: 779.7605\n",
            "-----------\n",
            "Train: Step: 900 Avg Loss: 6.5601 Avg Ppl: 803.1263 \n",
            "Val: Avg Loss: 6.6433 Avg Ppl: 768.1904\n",
            "Train: Step: 950 Avg Loss: 6.5478 Avg Ppl: 790.2803 \n",
            "Val: Avg Loss: 6.6604 Avg Ppl: 781.3397\n",
            "Train: Step: 1000 Avg Loss: 6.5370 Avg Ppl: 778.9092 \n",
            "Val: Avg Loss: 6.6349 Avg Ppl: 761.7858\n",
            "-----------\n",
            "Train: Step: 1050 Avg Loss: 6.5244 Avg Ppl: 767.2148 \n",
            "Val: Avg Loss: 6.6669 Avg Ppl: 786.5288\n",
            "Train: Step: 1100 Avg Loss: 6.5139 Avg Ppl: 756.9526 \n",
            "Val: Avg Loss: 6.6749 Avg Ppl: 792.8377\n",
            "-----------\n",
            "Train: Step: 1150 Avg Loss: 6.5031 Avg Ppl: 747.0824 \n",
            "Val: Avg Loss: 6.6687 Avg Ppl: 788.0067\n",
            "Train: Step: 1200 Avg Loss: 6.4928 Avg Ppl: 737.6714 \n",
            "Val: Avg Loss: 6.6768 Avg Ppl: 794.3111\n",
            "Train: Step: 1250 Avg Loss: 6.4833 Avg Ppl: 728.9860 \n",
            "Val: Avg Loss: 6.6616 Avg Ppl: 782.5082\n",
            "-----------\n",
            "Train: Step: 1300 Avg Loss: 6.4721 Avg Ppl: 719.9244 \n",
            "Val: Avg Loss: 6.6955 Avg Ppl: 809.3665\n",
            "Train: Step: 1350 Avg Loss: 6.4626 Avg Ppl: 711.7886 \n",
            "Val: Avg Loss: 6.6795 Avg Ppl: 796.4921\n",
            "-----------\n",
            "Train: Step: 1400 Avg Loss: 6.4529 Avg Ppl: 703.9855 \n",
            "Val: Avg Loss: 6.6861 Avg Ppl: 801.8265\n",
            "Train: Step: 1450 Avg Loss: 6.4441 Avg Ppl: 696.6595 \n",
            "Val: Avg Loss: 6.7066 Avg Ppl: 818.3088\n",
            "Train: Step: 1500 Avg Loss: 6.4359 Avg Ppl: 689.8756 \n",
            "Val: Avg Loss: 6.6995 Avg Ppl: 812.7539\n",
            "-----------\n",
            "Train: Step: 1550 Avg Loss: 6.4260 Avg Ppl: 682.5689 \n",
            "Val: Avg Loss: 6.7197 Avg Ppl: 829.2151\n",
            "Train: Step: 1600 Avg Loss: 6.4179 Avg Ppl: 676.1243 \n",
            "Val: Avg Loss: 6.7385 Avg Ppl: 844.8868\n",
            "-----------\n",
            "Train: Step: 1650 Avg Loss: 6.4093 Avg Ppl: 669.7444 \n",
            "Val: Avg Loss: 6.7507 Avg Ppl: 855.3951\n",
            "Train: Step: 1700 Avg Loss: 6.4012 Avg Ppl: 663.6281 \n",
            "Val: Avg Loss: 6.7393 Avg Ppl: 845.5841\n",
            "Train: Step: 1750 Avg Loss: 6.3937 Avg Ppl: 657.9342 \n",
            "Val: Avg Loss: 6.7181 Avg Ppl: 828.0823\n",
            "-----------\n",
            "Train: Step: 1800 Avg Loss: 6.3846 Avg Ppl: 651.7561 \n",
            "Val: Avg Loss: 6.7519 Avg Ppl: 856.5120\n",
            "Train: Step: 1850 Avg Loss: 6.3771 Avg Ppl: 646.2564 \n",
            "Val: Avg Loss: 6.7543 Avg Ppl: 858.3807\n",
            "-----------\n",
            "Train: Step: 1900 Avg Loss: 6.3690 Avg Ppl: 640.7545 \n",
            "Val: Avg Loss: 6.7882 Avg Ppl: 888.2577\n",
            "Train: Step: 1950 Avg Loss: 6.3618 Avg Ppl: 635.6341 \n",
            "Val: Avg Loss: 6.7820 Avg Ppl: 882.5259\n",
            "Train: Step: 2000 Avg Loss: 6.3548 Avg Ppl: 630.7171 \n",
            "Val: Avg Loss: 6.7879 Avg Ppl: 887.9801\n",
            "-----------\n",
            "Train: Step: 2050 Avg Loss: 6.3470 Avg Ppl: 625.6272 \n",
            "Val: Avg Loss: 6.7875 Avg Ppl: 887.5379\n",
            "Train: Step: 2100 Avg Loss: 6.3402 Avg Ppl: 620.9411 \n",
            "Val: Avg Loss: 6.7998 Avg Ppl: 898.3822\n",
            "-----------\n",
            "Train: Step: 2150 Avg Loss: 6.3330 Avg Ppl: 616.3140 \n",
            "Val: Avg Loss: 6.8085 Avg Ppl: 906.4986\n",
            "Train: Step: 2200 Avg Loss: 6.3264 Avg Ppl: 611.8567 \n",
            "Val: Avg Loss: 6.8026 Avg Ppl: 900.9299\n",
            "Train: Step: 2250 Avg Loss: 6.3200 Avg Ppl: 607.5814 \n",
            "Val: Avg Loss: 6.8038 Avg Ppl: 902.2617\n",
            "-----------\n",
            "Train: Step: 2300 Avg Loss: 6.3127 Avg Ppl: 603.1387 \n",
            "Val: Avg Loss: 6.8212 Avg Ppl: 917.8644\n",
            "Train: Step: 2350 Avg Loss: 6.3062 Avg Ppl: 598.9332 \n",
            "Val: Avg Loss: 6.8366 Avg Ppl: 932.0411\n",
            "-----------\n",
            "Train: Step: 2400 Avg Loss: 6.2994 Avg Ppl: 594.7991 \n",
            "Val: Avg Loss: 6.8563 Avg Ppl: 950.9267\n",
            "Train: Step: 2450 Avg Loss: 6.2931 Avg Ppl: 590.8231 \n",
            "Val: Avg Loss: 6.8475 Avg Ppl: 942.4785\n",
            "Train: Step: 2500 Avg Loss: 6.2869 Avg Ppl: 586.9333 \n",
            "Val: Avg Loss: 6.8615 Avg Ppl: 956.0330\n",
            "-----------\n",
            "Train: Step: 2550 Avg Loss: 6.2801 Avg Ppl: 582.9960 \n",
            "Val: Avg Loss: 6.8558 Avg Ppl: 950.2440\n",
            "Train: Step: 2600 Avg Loss: 6.2739 Avg Ppl: 579.2135 \n",
            "Val: Avg Loss: 6.8971 Avg Ppl: 990.2718\n",
            "-----------\n",
            "Train: Step: 2650 Avg Loss: 6.2673 Avg Ppl: 575.4469 \n",
            "Val: Avg Loss: 6.9174 Avg Ppl: 1010.8795\n",
            "Train: Step: 2700 Avg Loss: 6.2614 Avg Ppl: 571.8778 \n",
            "Val: Avg Loss: 6.8918 Avg Ppl: 985.2318\n",
            "Train: Step: 2750 Avg Loss: 6.2554 Avg Ppl: 568.3515 \n",
            "Val: Avg Loss: 6.9097 Avg Ppl: 1003.2767\n",
            "-----------\n",
            "Train: Step: 2800 Avg Loss: 6.2491 Avg Ppl: 564.8057 \n",
            "Val: Avg Loss: 6.8869 Avg Ppl: 980.2658\n",
            "Train: Step: 2850 Avg Loss: 6.2432 Avg Ppl: 561.3920 \n",
            "Val: Avg Loss: 6.9331 Avg Ppl: 1026.4542\n",
            "-----------\n",
            "Train: Step: 2900 Avg Loss: 6.2370 Avg Ppl: 558.0151 \n",
            "Val: Avg Loss: 6.9319 Avg Ppl: 1025.5786\n",
            "Train: Step: 2950 Avg Loss: 6.2313 Avg Ppl: 554.7534 \n",
            "Val: Avg Loss: 6.9413 Avg Ppl: 1035.1044\n",
            "Train: Step: 3000 Avg Loss: 6.2257 Avg Ppl: 551.5728 \n",
            "Val: Avg Loss: 6.9600 Avg Ppl: 1054.9364\n",
            "-----------\n",
            "Train: Step: 3050 Avg Loss: 6.2196 Avg Ppl: 548.3274 \n",
            "Val: Avg Loss: 6.9431 Avg Ppl: 1037.0960\n",
            "Train: Step: 3100 Avg Loss: 6.2140 Avg Ppl: 545.2035 \n",
            "Val: Avg Loss: 6.9505 Avg Ppl: 1044.5794\n",
            "Train: Step: 3150 Avg Loss: 6.2082 Avg Ppl: 542.1147 \n",
            "Val: Avg Loss: 6.9581 Avg Ppl: 1053.0429\n",
            "-----------\n",
            "Train: Step: 3200 Avg Loss: 6.2026 Avg Ppl: 539.1057 \n",
            "Val: Avg Loss: 6.9815 Avg Ppl: 1078.1456\n",
            "Train: Step: 3250 Avg Loss: 6.1974 Avg Ppl: 536.2054 \n",
            "Val: Avg Loss: 7.0028 Avg Ppl: 1100.9646\n",
            "-----------\n",
            "Train: Step: 3300 Avg Loss: 6.1917 Avg Ppl: 533.2833 \n",
            "Val: Avg Loss: 6.9877 Avg Ppl: 1084.5499\n",
            "Train: Step: 3350 Avg Loss: 6.1865 Avg Ppl: 530.4581 \n",
            "Val: Avg Loss: 7.0447 Avg Ppl: 1147.7833\n",
            "Train: Step: 3400 Avg Loss: 6.1815 Avg Ppl: 527.7730 \n",
            "Val: Avg Loss: 6.9855 Avg Ppl: 1082.4266\n",
            "-----------\n",
            "Train: Step: 3450 Avg Loss: 6.1760 Avg Ppl: 524.9701 \n",
            "Val: Avg Loss: 7.0097 Avg Ppl: 1108.4221\n",
            "Train: Step: 3500 Avg Loss: 6.1710 Avg Ppl: 522.3435 \n",
            "Val: Avg Loss: 7.0143 Avg Ppl: 1113.5311\n",
            "-----------\n",
            "Train: Step: 3550 Avg Loss: 6.1657 Avg Ppl: 519.6690 \n",
            "Val: Avg Loss: 7.0418 Avg Ppl: 1144.8948\n",
            "Train: Step: 3600 Avg Loss: 6.1608 Avg Ppl: 517.0988 \n",
            "Val: Avg Loss: 7.0771 Avg Ppl: 1186.3697\n",
            "Train: Step: 3650 Avg Loss: 6.1561 Avg Ppl: 514.6321 \n",
            "Val: Avg Loss: 7.0203 Avg Ppl: 1120.6849\n",
            "-----------\n",
            "Train: Step: 3700 Avg Loss: 6.1507 Avg Ppl: 512.0176 \n",
            "Val: Avg Loss: 7.0235 Avg Ppl: 1123.5925\n",
            "Train: Step: 3750 Avg Loss: 6.1460 Avg Ppl: 509.5914 \n",
            "Val: Avg Loss: 7.0319 Avg Ppl: 1133.2065\n",
            "-----------\n",
            "Train: Step: 3800 Avg Loss: 6.1408 Avg Ppl: 507.1007 \n",
            "Val: Avg Loss: 7.0894 Avg Ppl: 1200.9371\n",
            "Train: Step: 3850 Avg Loss: 6.1360 Avg Ppl: 504.6806 \n",
            "Val: Avg Loss: 7.0926 Avg Ppl: 1205.0518\n",
            "Train: Step: 3900 Avg Loss: 6.1314 Avg Ppl: 502.3599 \n",
            "Val: Avg Loss: 7.0550 Avg Ppl: 1160.4762\n",
            "-----------\n",
            "Train: Step: 3950 Avg Loss: 6.1261 Avg Ppl: 499.8975 \n",
            "Val: Avg Loss: 7.0794 Avg Ppl: 1188.6522\n",
            "Train: Step: 4000 Avg Loss: 6.1215 Avg Ppl: 497.5960 \n",
            "Val: Avg Loss: 7.0924 Avg Ppl: 1204.0793\n",
            "-----------\n",
            "Train: Step: 4050 Avg Loss: 6.1165 Avg Ppl: 495.2690 \n",
            "Val: Avg Loss: 7.0851 Avg Ppl: 1195.5595\n",
            "Train: Step: 4100 Avg Loss: 6.1119 Avg Ppl: 493.0182 \n",
            "Val: Avg Loss: 7.1520 Avg Ppl: 1278.6041\n",
            "Train: Step: 4150 Avg Loss: 6.1075 Avg Ppl: 490.8505 \n",
            "Val: Avg Loss: 7.1147 Avg Ppl: 1231.9527\n",
            "-----------\n",
            "Train: Step: 4200 Avg Loss: 6.1027 Avg Ppl: 488.6364 \n",
            "Val: Avg Loss: 7.1377 Avg Ppl: 1260.1788\n",
            "Train: Step: 4250 Avg Loss: 6.0982 Avg Ppl: 486.4861 \n",
            "Val: Avg Loss: 7.1303 Avg Ppl: 1250.5383\n",
            "-----------\n",
            "Train: Step: 4300 Avg Loss: 6.0936 Avg Ppl: 484.3777 \n",
            "Val: Avg Loss: 7.1383 Avg Ppl: 1261.1836\n",
            "Train: Step: 4350 Avg Loss: 6.0894 Avg Ppl: 482.3355 \n",
            "Val: Avg Loss: 7.1941 Avg Ppl: 1334.1127\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-5ed602aadacd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Now we have to write the training loop \n",
        "\n",
        "model = Transformer().to(device) \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "total_loss = 0 \n",
        "total_ppl = 0 \n",
        "log_interval = 50\n",
        "\n",
        "# Eval function to eval on dev set \n",
        "\n",
        "def eval():\n",
        "  step = 0 \n",
        "  total_loss = 0 \n",
        "  total_ppl = 0 \n",
        "  model.eval() \n",
        "  # Sample from eval set \n",
        "  with torch.no_grad():\n",
        "    for i in range(0, val_data.shape[0], bptt):\n",
        "      step += 1 \n",
        "      inputs, targets = get_data(val_data, i) \n",
        "      inputs, targets = inputs.to(device), targets.to(device) \n",
        "      mask = get_mask(min(bptt, len(val_data) -i -1)).to(device) \n",
        "\n",
        "      outputs = model(inputs, attn_mask=mask).reshape(-1, vocab_size)\n",
        "      loss = criterion(outputs, targets)\n",
        "      total_loss += loss.item() \n",
        "      total_ppl += math.exp(loss.item()) \n",
        "      avg_loss = total_loss / step \n",
        "      avg_ppl = total_ppl / step \n",
        "\n",
        "  print(\"Val: Avg Loss: {:.4f} Avg Ppl: {:.4f}\".format(avg_loss, avg_ppl))\n",
        "\n",
        "model.train() \n",
        "step = 0 \n",
        "# Loop from start of the dataset to the final index with increments of bptt window size \n",
        "for epoch in range(1, 101):\n",
        "  for i in range(0, train_data.shape[0], bptt):\n",
        "    step += 1 \n",
        "    # Get inputs and targets \n",
        "    inputs, targets = get_data(train_data, i)\n",
        "    # Move to device \n",
        "    inputs, targets = inputs.to(device), targets.to(device) \n",
        "    mask = get_mask(min(bptt, len(train_data) -i - 1)).to(device) \n",
        "\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    outputs = model(inputs, attn_mask=mask).reshape(-1, vocab_size)\n",
        "    loss = criterion(outputs, targets) \n",
        "\n",
        "    # backward pass \n",
        "    loss.backward() \n",
        "    optimizer.step()\n",
        "    \n",
        "    total_loss += loss.item() \n",
        "    total_ppl += math.exp(loss.item())\n",
        "    if step % log_interval == 0:\n",
        "      avg_loss = total_loss / step \n",
        "      avg_ppl = total_ppl / step \n",
        "      print(\"Train: Step: {} Avg Loss: {:.4f} Avg Ppl: {:.4f} \".format(step, avg_loss, avg_ppl))\n",
        "      eval() # Evalvulate on val set \n",
        "  print(\"-----------\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "LET'S UNDERSTAND PADDING AND PAD MASK \n",
        "\"\"\"\n",
        "def pad_sentence(token_list, pad_id, max_len):\n",
        "  \"\"\"\n",
        "  Pad the incoming token list to bptt window size \n",
        "  \"\"\"\n",
        "  sentence = [pad_token_id for _ in range(max_len)]\n",
        "  sentence[:len(token_list)] = token_list\n",
        "  return sentence \n",
        "\n",
        "def get_pad_mask(sentence, pad_token_id):\n",
        "  \"\"\"\n",
        "  We need a mask to make sure the padded elements are contributing to calculations \n",
        "  \"\"\"\n",
        "  return (torch.tensor(sentence) == torch.tensor(pad_token_id))\n",
        "\n",
        "# Test the pad mask \n",
        "prompt = \"hello hi how are\"\n",
        "pad_token_id = v.stoi([\"<pad>\"])[0]\n",
        "tokenized_prompt = tokenize(prompt)\n",
        "# convert to ids \n",
        "indices = v.stoi(tokenized_prompt)\n",
        "padded_sentence = pad_sentence(indices, pad_token_id, 8)\n",
        "print(padded_sentence)\n",
        "\n",
        "# The last 4 tokens are pad tokens, they must have 0 embeddings to not \n",
        "# contribute to the attention calc \n",
        "rand_embeddings = torch.rand(8, 4)\n",
        "m = get_pad_mask(padded_sentence, pad_token_id).unsqueeze(1)\n",
        "print(rand_embeddings)\n",
        "print(rand_embeddings.masked_fill_(m == 1, 0))"
      ],
      "metadata": {
        "id": "BW6eL-VliBlj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01d054bb-860d-483d-924f-9148813c9196"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 434, 36, 1, 1, 1, 1]\n",
            "tensor([[0.3615, 0.3252, 0.4443, 0.7291],\n",
            "        [0.0801, 0.1024, 0.1770, 0.0743],\n",
            "        [0.0281, 0.0741, 0.9551, 0.9070],\n",
            "        [0.1991, 0.6719, 0.6988, 0.9431],\n",
            "        [0.7283, 0.7808, 0.8266, 0.7172],\n",
            "        [0.2607, 0.6559, 0.5709, 0.8496],\n",
            "        [0.2036, 0.2674, 0.8511, 0.7719],\n",
            "        [0.8451, 0.3281, 0.6048, 0.5914]])\n",
            "tensor([[0.3615, 0.3252, 0.4443, 0.7291],\n",
            "        [0.0801, 0.1024, 0.1770, 0.0743],\n",
            "        [0.0281, 0.0741, 0.9551, 0.9070],\n",
            "        [0.1991, 0.6719, 0.6988, 0.9431],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Padded Sentence -- [0, 0, 434, 36, 1, 1, 1, 1]\n",
        "\n",
        "Assume these are the embeddings: \n",
        "Each embedding of dimension 4 for 8 tokens \n",
        "\n",
        "tensor([[0.7591, 0.4311, 0.6974, 0.6955],\n",
        "        [0.9180, 0.0609, 0.8108, 0.4425],\n",
        "        [0.2840, 0.5337, 0.9412, 0.9352],\n",
        "        [0.0751, 0.4733, 0.2398, 0.3400],\n",
        "        [0.7350, 0.3453, 0.3073, 0.0521],\n",
        "        [0.8824, 0.3623, 0.0736, 0.8674],\n",
        "        [0.6345, 0.2537, 0.6060, 0.7582],\n",
        "        [0.2599, 0.4035, 0.2184, 0.7458]])\n",
        "\n",
        "The pad tokens (last 4 tokens) don't contribute in attention calc now.\n",
        "\n",
        "tensor([[0.7591, 0.4311, 0.6974, 0.6955],\n",
        "        [0.9180, 0.0609, 0.8108, 0.4425],\n",
        "        [0.2840, 0.5337, 0.9412, 0.9352],\n",
        "        [0.0751, 0.4733, 0.2398, 0.3400],\n",
        "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
        "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
        "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
        "        [0.0000, 0.0000, 0.0000, 0.0000]])\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "aaD6sGJumyat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Language generation\n",
        "\n",
        "Okay. Now that we trained our model, we want to get to the fun stuff -- generating it with prompts. \n",
        "Let's start with a simple greedy pick strategy, that is, push each token through the model and \n",
        "select the most probable word each time. \n",
        "As I understand it, the pseudo code for generation is gonna be something like \n",
        "\n",
        "generated_token = <start> \n",
        "input = [generated_token..<pad>..<pad>] # shape - 1 x max_seq_length\n",
        "while (generated_token != <end>)\n",
        "  mask = pad_mask && autoregressive mask # combine both masks \n",
        "  prob_dist_over_vocab = model(input, mask)\n",
        "  generated_token = sample(prob_dist_over_vocab) \n",
        "  input[i] = generated_token_id \n",
        "\n",
        "final test = id_to_token(input)\n",
        "\"\"\"\n",
        "generation_steps = 100 \n",
        "prompt = \"hello this is a prompt to test\"\n",
        "# Tokenized \n",
        "tokenized_prompt = tokenize(prompt)\n",
        "print(tokenized_prompt)\n",
        "# convert to ids \n",
        "indices = v.stoi(tokenized_prompt)\n",
        "total_log_prob = 0 \n",
        "\n",
        "for _ in range(generation_steps):\n",
        "  input = torch.tensor(indices, dtype=torch.long).unsqueeze(0) .to(device)\n",
        "  # Now let's make a forward pass and get some predictions \n",
        "  seq_len = len(indices)\n",
        "  mask = get_mask(seq_len).to(device)\n",
        "  preds = model(input, attn_mask=mask)\n",
        "  probs = preds[:, -1, :]\n",
        "  index = torch.argmax(probs, dim=-1).cpu().numpy()[0]\n",
        "\n",
        "  indices.append(index)\n",
        "  prompt += \" \" + str(v.itos([index])[0])\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAMojRJLenOX",
        "outputId": "eaf94f4f-f597-4011-b78a-584b4638e2f5"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'this', 'is', 'a', 'prompt', 'to', 'test']\n",
            "hello this is a prompt to test Tong <unk> <unk> <unk> <unk> Hands legs g advancing No <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Autoregressive Language Modeling",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}